{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.models import BaseRNN, BaseLSTM, DoubleLSTM\n",
    "from lib.train import train, evaluate\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_tfrecords_from_raw_data(raw_data_dir='data/raw_data',tf_rec_data_dir='data/processed_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "session_name = 'lstm_5_1_small_batch_weights_test' +  '_' + current_time\n",
    "optimizer_params = {'learning_rate':0.0001}\n",
    "training_params = {'num_epochs':100, 'batch_size':64,'apnea_weight':5}\n",
    "preprocess_params = {'featurize_func' : utils.featurize_2,'seq_len':30,'pulse_sample_rate':16,'data_stride':15}\n",
    "preprocess_func = utils.preprocess_data\n",
    "model_params = {'rnn_hidden_dim':20}\n",
    "test_split = .15\n",
    "val_split = 0.1765\n",
    "\n",
    "model = BaseLSTM(rnn_hidden_dim=model_params['rnn_hidden_dim'])\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(optimizer_params['learning_rate'])\n",
    "log_dir = 'logs/gradient_tape/' + session_name\n",
    "model_weights_dir = 'model_weights/' + session_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0831 11:02:46.983357 4416091584 deprecation.py:323] From /anaconda3/envs/apnea/lib/python3.6/site-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57911\n",
      "0.090015136\n",
      "6365\n",
      "0.094920136\n",
      "6571\n",
      "0.0513519\n"
     ]
    }
   ],
   "source": [
    "dataset = utils.load_tfrecords(tf_rec_data_dir='data/processed_data')\n",
    "num_records = 0\n",
    "for i in dataset:\n",
    "    num_records += 1\n",
    "\n",
    "train_data, test_data = utils.split_dataset(dataset, test_split)\n",
    "train_data, val_data = utils.split_dataset(train_data, val_split)\n",
    "train_data = preprocess_func(train_data, \n",
    "                             featurize_func = preprocess_params['featurize_func'],\n",
    "                             seq_len=preprocess_params['seq_len'], \n",
    "                             pulse_sample_rate=preprocess_params['pulse_sample_rate'],\n",
    "                             data_stride=preprocess_params['data_stride'])\n",
    "val_data = preprocess_func(val_data, \n",
    "                           featurize_func = preprocess_params['featurize_func'],\n",
    "                           seq_len=preprocess_params['seq_len'], \n",
    "                           pulse_sample_rate=preprocess_params['pulse_sample_rate'],\n",
    "                           data_stride=preprocess_params['seq_len'])\n",
    "test_data = preprocess_func(test_data, \n",
    "                            featurize_func = preprocess_params['featurize_func'],\n",
    "                            seq_len=preprocess_params['seq_len'], \n",
    "                            pulse_sample_rate=preprocess_params['pulse_sample_rate'],\n",
    "                            data_stride=preprocess_params['seq_len'])\n",
    "\n",
    "train_num = train_data._tensors[0].shape[0]\n",
    "train_bal = train_data._tensors[1].numpy().mean()\n",
    "val_num = val_data._tensors[0].shape[0]\n",
    "val_bal = val_data._tensors[1].numpy().mean()\n",
    "test_num = test_data._tensors[0].shape[0]\n",
    "test_bal = test_data._tensors[1].numpy().mean()\n",
    "\n",
    "data_bal = {'train':train_bal,'val':val_bal,'test':test_bal}\n",
    "data_size = {'train':train_num,'val':val_num,'test':test_num}\n",
    "print(train_num)\n",
    "print(train_bal)\n",
    "print(val_num)\n",
    "print(val_bal)\n",
    "print(test_num)\n",
    "print(test_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data,val_data, loss_object, optimizer, log_dir,model_weights_dir,\n",
    "      num_epochs=training_params['num_epochs'], \n",
    "      batch_size=training_params['batch_size'],\n",
    "      apnea_weight=training_params['apnea_weight'])\n",
    "train_res = evaluate(model, train_data)\n",
    "test_res = evaluate(model, test_data)\n",
    "print(train_res)\n",
    "print(test_res)\n",
    "utils.save_session(session_name,\n",
    "                   model,\n",
    "                   model_params,\n",
    "                   num_records,\n",
    "                   test_split,\n",
    "                   val_split,\n",
    "                   preprocess_func,\n",
    "                   preprocess_params,\n",
    "                   data_bal,\n",
    "                   data_size,\n",
    "                   training_params,\n",
    "                   optimizer,\n",
    "                   optimizer_params,\n",
    "                   train_res,\n",
    "                   test_res,\n",
    "                   log_dir,\n",
    "                   model_weights_dir,\n",
    "                   res_path = 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8405341712490181,\n",
       " 'recall': 0.4867310344827586,\n",
       " 'precision': 0.2943708498782075,\n",
       " 'f1': 0.36686488959121727,\n",
       " 'auc': 0.7892316308803503}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    \n",
    "for x_batch,y_batch in train_data.batch(128, drop_remainder=True):\n",
    "    pass\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 predictions = tf.reshape(model(x_batch),[-1])\n",
    "#                 labels = tf.reshape(y_batch,[-1])\n",
    "#                 sample_weight = tf.convert_to_tensor(labels.numpy()*apnea_weight)\n",
    "#                                 loss = loss_object(labels, predictions, sample_weight=sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.rank(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.rank(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(sample_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.expand_dims(tf.reshape(model(x_batch),[-1]),1)\n",
    "labels = tf.expand_dims(tf.reshape(y_batch,[-1]),1)\n",
    "print(labels.shape)\n",
    "\n",
    "sample_weight = tf.convert_to_tensor(labels.numpy()*5)\n",
    "print(sample_weight.shape)\n",
    "loss = loss_object(labels, predictions, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.utils import to_dense_tensors\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = load_tfrecords(tf_rec_data_dir='data/processed_data')\n",
    "dataset = dataset.map(to_dense_tensors)\n",
    "for dp in dataset:\n",
    "    pass\n",
    "x = dp['x']\n",
    "y = dp['y']\n",
    "plt.plot((x - tf.math.reduce_mean(x)) / tf.math.reduce_std(x))\n",
    "plt.plot(np.repeat(y,16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulse_sample_rate = 16\n",
    "data_stride = 30\n",
    "seq_len = 16\n",
    "X = []\n",
    "Y = []\n",
    "#normalize pulse data by night of sleep\n",
    "x = (x - tf.math.reduce_mean(x)) / tf.math.reduce_std(x)\n",
    "#convert night of sleep from 1 dimensional sequence of num samples length\n",
    "#to a sequence of pulse_sample_rate dimension of num seconds length\n",
    "num_seconds = x.shape[0]//pulse_sample_rate\n",
    "x_trunc = x[:pulse_sample_rate*(num_seconds)]\n",
    "x = tf.reshape(x_trunc,[num_seconds,pulse_sample_rate])\n",
    "#create new datapoints, according to data_stride\n",
    "num_data_points = (x.shape[0]//data_stride) - (seq_len//data_stride)\n",
    "for i in range(0,num_data_points):\n",
    "    X.append(x[data_stride*i:data_stride*i+seq_len])\n",
    "    Y.append(y[data_stride*i:data_stride*i+seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.absolute(x[data_stride*i:data_stride*i+seq_len].numpy().flatten()) > 1.5).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x[data_stride*i:data_stride*i+seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[data_stride*i:data_stride*i+seq_len].numpy().mean() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo and Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix learning rate\n",
    "#visualize function\n",
    "#develop model search procedure - preprocessing, architecture, training params\n",
    "#set seeds\n",
    "\n",
    "#remove outliers\n",
    "#0-1 scaling \n",
    "\n",
    "\n",
    "#write main.py with argparse\n",
    "#write shell script for parameter tuning\n",
    "#set up interpretion, eda notebook\n",
    "#turn model params into keyword args\n",
    "#add metrics to tensorboard\n",
    "#add keyboard interupt to training loop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
