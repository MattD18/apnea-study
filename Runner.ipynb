{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# project_dir = os.path.join('drive','My Drive','Personal Projects','Sleep Apnea Study','apnea-study')\n",
    "# os.chdir(project_dir)\n",
    "# !git pull\n",
    "# !pip install -r colab_requirements.txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.models import BaseRNN, BaseLSTM, DoubleLSTM\n",
    "from lib.train import train, evaluate\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homepap-lab-full-1600187 has sample rate of 64, skipping\n",
      "homepap-lab-full-1600192 has sample rate of 200, skipping\n",
      "homepap-lab-full-1600186 has sample rate of 200, skipping\n",
      "homepap-lab-full-1600191 does not track pulse, skipping\n",
      "homepap-lab-full-1600188 has sample rate of 200, skipping\n",
      "homepap-lab-full-1600189 has sample rate of 200, skipping\n"
     ]
    }
   ],
   "source": [
    "utils.create_tfrecords_from_raw_data(raw_data_dir='data/raw_data',tf_rec_data_dir='data/processed_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "session_name = 'more_data_test' +  '_' + current_time\n",
    "optimizer_params = {'learning_rate':0.0001}\n",
    "training_params = {'num_epochs':100, 'batch_size':256,'apnea_weight':10}\n",
    "preprocess_params = {'featurize_func' : utils.featurize_2,'seq_len':60,'pulse_sample_rate':16,'data_stride':15, 'split_seed':2}\n",
    "preprocess_func = utils.preprocess_data\n",
    "model_params = {'rnn_hidden_dim':20}\n",
    "test_split = .15\n",
    "val_split = 0.1765\n",
    "if tf.test.is_gpu_available():\n",
    "    print('Using GPU')\n",
    "    with tf.device(\"gpu:0\"):\n",
    "        model = BaseLSTM(rnn_hidden_dim=model_params['rnn_hidden_dim'])\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "        optimizer = tf.keras.optimizers.Adam(optimizer_params['learning_rate'])\n",
    "else:\n",
    "    model = BaseLSTM(rnn_hidden_dim=model_params['rnn_hidden_dim'])\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(optimizer_params['learning_rate'])\n",
    "log_dir = 'logs/gradient_tape/' + session_name\n",
    "model_weights_dir = 'model_weights/' + session_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# print(tf.test.is_gpu_available())\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76510\n",
      "0.061002046\n",
      "3925\n",
      "0.020450106\n",
      "4079\n",
      "0.051716108\n"
     ]
    }
   ],
   "source": [
    "dataset = utils.load_tfrecords(tf_rec_data_dir='data/processed_data')\n",
    "num_records = 0\n",
    "for i in dataset:\n",
    "    num_records += 1\n",
    "\n",
    "train_data, test_data = utils.split_dataset(dataset, test_split,seed=preprocess_params['split_seed'])\n",
    "train_data, val_data = utils.split_dataset(train_data, val_split,seed=preprocess_params['split_seed'])\n",
    "train_data = preprocess_func(train_data, \n",
    "                             featurize_func = preprocess_params['featurize_func'],\n",
    "                             seq_len=preprocess_params['seq_len'], \n",
    "                             pulse_sample_rate=preprocess_params['pulse_sample_rate'],\n",
    "                             data_stride=preprocess_params['data_stride'])\n",
    "val_data = preprocess_func(val_data, \n",
    "                           featurize_func = preprocess_params['featurize_func'],\n",
    "                           seq_len=preprocess_params['seq_len'], \n",
    "                           pulse_sample_rate=preprocess_params['pulse_sample_rate'],\n",
    "                           data_stride=preprocess_params['seq_len'])\n",
    "test_data = preprocess_func(test_data, \n",
    "                            featurize_func = preprocess_params['featurize_func'],\n",
    "                            seq_len=preprocess_params['seq_len'], \n",
    "                            pulse_sample_rate=preprocess_params['pulse_sample_rate'],\n",
    "                            data_stride=preprocess_params['seq_len'])\n",
    "\n",
    "train_num = train_data._tensors[0].shape[0]\n",
    "train_bal = train_data._tensors[1].numpy().mean()\n",
    "val_num = val_data._tensors[0].shape[0]\n",
    "val_bal = val_data._tensors[1].numpy().mean()\n",
    "test_num = test_data._tensors[0].shape[0]\n",
    "test_bal = test_data._tensors[1].numpy().mean()\n",
    "\n",
    "data_bal = {'train':train_bal,'val':val_bal,'test':test_bal}\n",
    "data_size = {'train':train_num,'val':val_num,'test':test_num}\n",
    "print(train_num)\n",
    "print(train_bal)\n",
    "print(val_num)\n",
    "print(val_bal)\n",
    "print(test_num)\n",
    "print(test_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "{'accuracy': 0.8041391103559448, 'recall': 0.561638503620963, 'precision': 0.16845768341364614, 'f1': 0.2591776889567817, 'auc': 0.7759761172868587}\n",
      "{'accuracy': 0.8206954318869004, 'recall': 0.43643833451844827, 'precision': 0.13067133462648436, 'f1': 0.20112504778722395, 'auc': 0.6955351145817305}\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data,val_data, loss_object, optimizer, log_dir,model_weights_dir,\n",
    "      num_epochs=training_params['num_epochs'], \n",
    "      batch_size=training_params['batch_size'],\n",
    "      apnea_weight=training_params['apnea_weight'])\n",
    "train_res = evaluate(model, train_data)\n",
    "test_res = evaluate(model, test_data)\n",
    "print(train_res)\n",
    "print(test_res)\n",
    "utils.save_session(session_name,\n",
    "                   model,\n",
    "                   model_params,\n",
    "                   num_records,\n",
    "                   test_split,\n",
    "                   val_split,\n",
    "                   preprocess_func,\n",
    "                   preprocess_params,\n",
    "                   data_bal,\n",
    "                   data_size,\n",
    "                   training_params,\n",
    "                   optimizer,\n",
    "                   optimizer_params,\n",
    "                   train_res,\n",
    "                   test_res,\n",
    "                   log_dir,\n",
    "                   model_weights_dir,\n",
    "                   res_path = 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8116942675159236,\n",
       " 'recall': 0.2664036544850498,\n",
       " 'precision': 0.03047795515013303,\n",
       " 'f1': 0.054698158253751704,\n",
       " 'auc': 0.5779425461588421}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    \n",
    "for x_batch,y_batch in train_data.batch(128, drop_remainder=True):\n",
    "    pass\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 predictions = tf.reshape(model(x_batch),[-1])\n",
    "#                 labels = tf.reshape(y_batch,[-1])\n",
    "#                 sample_weight = tf.convert_to_tensor(labels.numpy()*apnea_weight)\n",
    "#                                 loss = loss_object(labels, predictions, sample_weight=sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.rank(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.rank(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(sample_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.expand_dims(tf.reshape(model(x_batch),[-1]),1)\n",
    "labels = tf.expand_dims(tf.reshape(y_batch,[-1]),1)\n",
    "print(labels.shape)\n",
    "\n",
    "sample_weight = tf.convert_to_tensor(labels.numpy()*5)\n",
    "print(sample_weight.shape)\n",
    "loss = loss_object(labels, predictions, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.utils import to_dense_tensors\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = load_tfrecords(tf_rec_data_dir='data/processed_data')\n",
    "dataset = dataset.map(to_dense_tensors)\n",
    "for dp in dataset:\n",
    "    pass\n",
    "x = dp['x']\n",
    "y = dp['y']\n",
    "plt.plot((x - tf.math.reduce_mean(x)) / tf.math.reduce_std(x))\n",
    "plt.plot(np.repeat(y,16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulse_sample_rate = 16\n",
    "data_stride = 30\n",
    "seq_len = 16\n",
    "X = []\n",
    "Y = []\n",
    "#normalize pulse data by night of sleep\n",
    "x = (x - tf.math.reduce_mean(x)) / tf.math.reduce_std(x)\n",
    "#convert night of sleep from 1 dimensional sequence of num samples length\n",
    "#to a sequence of pulse_sample_rate dimension of num seconds length\n",
    "num_seconds = x.shape[0]//pulse_sample_rate\n",
    "x_trunc = x[:pulse_sample_rate*(num_seconds)]\n",
    "x = tf.reshape(x_trunc,[num_seconds,pulse_sample_rate])\n",
    "#create new datapoints, according to data_stride\n",
    "num_data_points = (x.shape[0]//data_stride) - (seq_len//data_stride)\n",
    "for i in range(0,num_data_points):\n",
    "    X.append(x[data_stride*i:data_stride*i+seq_len])\n",
    "    Y.append(y[data_stride*i:data_stride*i+seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.absolute(x[data_stride*i:data_stride*i+seq_len].numpy().flatten()) > 1.5).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x[data_stride*i:data_stride*i+seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[data_stride*i:data_stride*i+seq_len].numpy().mean() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo and Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix learning rate\n",
    "#visualize function\n",
    "#develop model search procedure - preprocessing, architecture, training params\n",
    "#set seeds\n",
    "#teacher forcing /embed output, sequence to sequence model\n",
    "#bidirectional\n",
    "\n",
    "#remove outliers\n",
    "#0-1 scaling \n",
    "\n",
    "\n",
    "#write main.py with argparse\n",
    "#write shell script for parameter tuning\n",
    "#set up interpretion, eda notebook\n",
    "#turn model params into keyword args\n",
    "#add metrics to tensorboard\n",
    "#add keyboard interupt to training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
